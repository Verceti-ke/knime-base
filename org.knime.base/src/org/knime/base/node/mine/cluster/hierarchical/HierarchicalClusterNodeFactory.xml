<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 1.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="./dendrogram.png" type="Learner">
	<name>Hierarchical Clustering</name>
	<shortDescription>
		Performs Hierarchical Clustering.
	</shortDescription>
	<fullDescription>
		<intro>
		<p>
		Hierarchically clusters the input data. <br />
		Note: This node works only on small data sets. It keeps the entire data
		in memory and has a n-squared complexity.<br />
		There are two methods to do hierarchical clustering:
		<ul>
			<li>
			Top-down or divisive, i.e. the algorithm starts with all data 
			points in one huge cluster and the most dissimilar datapoints are 
			divided into subclusters until every data point is in exactly one 
			cluster.
			</li>
			<li>
			Bottom-up or agglomerative, i.e. the algorithm starts with every 
			datapoint as one single cluster and tries to combine the most similar 
			ones into superclusters until it ends up in one huge cluster containing 
			all subclusters.
			</li>
		</ul>
		This algorithm works agglomerative. 
		</p>
		<p>In order to determine the distance 
		between clusters a measure has to be defined. Basically, there exist 
		three methods to compare two clusters:
		<ul>
		<li>Single Linkage: defines the distance between two clusters c1 and c2 
		as the minimal distance between any two points x, y with x in c1 and y in c2.</li>
		<li>Complete Linkage: defines the distance between two clusters c1 and c2 
		as the maximal distance between any two points x, y with x in c1 and y in c2.</li>
		<li>Average Linkage: defines the distance between two clusters c1 and c2 
		as the mean distance between all points in c1 and c2.</li>
		</ul>
		</p>
		<p>
		In order to measure the distance between two points a distance measure is necessary. 
		You can choose between the manhattan distance and the euclidean distance, 
		which corresponds to the L1 and the L2 norm.
		</p>
		<p>
		The output is the same data as the input with one additional column with
		the clustername the data point is assigned to. Since a hierarchical 
		clustering algorithm produces a series of cluster results, the number of 
		clusters for the output has to be defined in the dialog.
		</p>
		<p>
			Views:
			<ul>
			<li>Dendrogram:
			The view shows a dendrogram which displayes the whole cluster 
			hierarchy. At the bottom are all datapoints. The closest data points are connected 
			where the height of the connection shows the distance between them. 
			Each cluster can be selected and hilited. All contained subclusters will be hilited, too.
			</li>
			<li>Distance plot:
			The distance plot displays the distances between the cluster for each
			number of clusters. This view can help to determine a "good" number of clusters, since 
			there will be sudden jumps in the level of similarity as dissimilar groups are fused.
			</li>
			</ul>
			</p>
		</intro>
		<option name="Number output cluster">Which level of the hierarchy to use 
			for the output column.</option>
		<option name="Distance function">Which distance measure to use for the 
			distance between points.</option>
		<option name="Linkage type">Which method to use to measure the distance 
		between points (as described above)</option>
        <option name="Distance cache">Caching the distances between the data points
        drastically improves performance especially for high-dimensional datasets. However, it needs
        much memory, so you can switch it off for large datasets.</option>
	</fullDescription>
	<ports>
	<dataIn index="0" name="Data to cluster">
			The data that should be clustered using hierarchical clustering. 
			Only numeric columns are considered, nominal columns are ignored.
	</dataIn>
	<dataOut index="0" name="Clustered data">
		The input data with an extra column with the cluster name where the data point is assinged to.
	</dataOut>
	</ports>	
	<views>
		<view index="0" name="Dendrogram/Distance View">
		</view>
	</views>
	
</knimeNode>
