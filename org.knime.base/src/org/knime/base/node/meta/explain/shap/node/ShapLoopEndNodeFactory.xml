<?xml version="1.0" encoding="utf-8"?>
<knimeNode icon="explainer_icon.png" type="LoopEnd"
	xmlns="http://knime.org/node/v3.6"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://knime.org/node/v3.6 http://knime.org/node/v3.6.xsd">
	<name>SHAP Loop End</name>

	<shortDescription>This node marks the end of a SHAP loop.
	</shortDescription>

	<fullDescription>
		<intro>
			Calculates the SHAP values by evaluating the predictions your model made in the loop body.
			The output table of this node contains for each explained row of interest (rows in the first input table of the SHAP Loop Start node, we will refer to these as ROI)
			 <i>d</i> rows where <i>d</i> is the number of
			predictions your model produces (e.g. one for each class probability in a classification task).
			The rows consist of four special columns followed by a column for each of your features which hold the SHAP value of that feature for the current prediction column.
			The special columns are:
			<ul>
				<li>RowId: Holds the RowId of the explained ROI.</li>
				<li>Target: The name of the prediction column that is explained.</li>
				<li>Actual Prediction: The actual prediction for the unaltered ROI.</li>
				<li>Deviation from mean prediction: How much the prediction for this ROI differs from the mean prediction on the sampling table (second input table of the SHAP Loop Start).</li>
			</ul>
			The SHAP value for a single feature indicates how much this feature contributed to the deviation from the mean prediction.
			Provided the explanation set size is large enough, the values for all features should add up to the deviation from the mean prediction.
		</intro>
		
		<tab name="Options">
			<option name="Automatically detect prediction columns">
				With this option SHAP will use all numeric columns that were added in the loop body as prediction columns.
			</option>
			<option name="Manually select prediction column">
				Allows to manually select the prediction columns among the input columns of this node.
			</option>
			<option name="Regularize explanations">
				SHAP calculates the contributions of all features to the prediction but especially in cases where there are many features, it is oftentimes important to get concise explanations
				that only consider the most important features (i.e. those features that have the most influence on the prediction).
				If this box is checked, SHAP will use a linear model with elastic-net regularization to find the features that have the strongest influence on the predictions.
			</option>
			<option name="Maximum number of active features">
				The number of features you want to have participating in your explanations. The remaining features will receive SHAP values of 0.
			</option>
			<option name="Alpha">
				The elastic-net regularization is a mixture of L1 (Lasso) and L2 (Ridge) regularization where <i>alpha</i> controls the balance of the two.
				Larger <i>alpha</i> mean a stronger contribution of the L1 regularization.
			</option>
			<option name="Use element names for collection features">
				Collection and vector columns can store names for the individual elements they contain.
				By checking this box, these names will be used in the explanations produced by this node.
				Otherwise the columns corresponding to collection features will be named according to their collection followed by an index.
				Note that we also use the latter naming strategy if the number of names stored in a collection column is different from the
				number of elements in the collection.
			</option>
		</tab>
	</fullDescription>

	<ports>
		<inPort index="0" name="Table containing the rows to explain.">
			Table containing predictions for the perturbed rows produced by the Shapley Values Loop Start node
		</inPort>
		<outPort index="0" name="Table containing the Shapley Values.">Table containing the Shapley Values for each feature-prediction combination</outPort>
	</ports>
</knimeNode>
