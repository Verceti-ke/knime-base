<?xml version="1.0" encoding="utf-8"?>
<knimeNode icon="explainer_icon.png" type="LoopStart"
	xmlns="http://knime.org/node/v3.6"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://knime.org/node/v3.6 http://knime.org/node/v3.6.xsd">
	<name>Shapley Values Loop Start</name>

	<shortDescription>Start node for a Shapley Values loop
	</shortDescription>

	<fullDescription>
		<intro>
		<p>
			Shapley Values originate in game theory and in the context of machine learning they recently became a popular
			tool for the explanation of model predictions.
			The Shapley Value of a feature for a certain row and prediction indicates how much the feature contributed to
			the deviation of the prediction from the base prediction (i.e. the mean prediction over the full sampling data).
			In theory the Shapley Values of all features add up to the difference between the mean prediction and the actual prediction
			but this loop only produces approximations because it is typically infeasible to calculate the exact Shapley Values.
		</p>
		
		<h4>Usage</h4>
		<p>
			A typical Shapley Values loop will consist of only three nodes: The Shapley Values Loop Start node, the predictor node for the model you want to explain (e.g. a Random Forest Predictor node) and
			the Shapley Values Loop End node.
		</p>
		<p>
			The Shapley Values Loop Start node creates for each row in the ROI (Row of Interest) table a number of perturbed rows i.e. rows where some of the features are randomly exchanged with the features from rows in the sampling table
			(for the exact details of the algorithm we refer to algorithm one in the paper <i>Explaining prediction models and individual predictions with feature contributions</i>
			by Strumbelj and Kononenko).
			Your task is to obtain predictions for these permuted rows (usually via the Predictor node corresponding to your model).
			The Shapley Values Loop End node collects these predictions and calculates an approximation of the Shapley Values for each feature-target combination.
		</p>
		<h4>A note on collections and vectors</h4>
		<p>
			These nodes support collection and vector columns such as List columns, Bit Vector and Byte Vector columns, in case of which each element
			of the position/vector can be treated as individual feature. Note that this requires all collections/vectors in a single column to be of the
			same length i.e. contain the same number of elements.
			It is also possible to treat collections and vectors as single feature, in which case the respective option has to be set in the dialog.
		</p>
			
		</intro>
		
		<tab name="Options">
			<option name="Feature columns">
				The columns that contain features your model requires.
				Note that it is not allowed to have columns both as feature and prediction column.
			</option>
			<option name="Every collection column represents a single feature">
				If checked, collection and vector columns are treated as a whole, i.e. either the whole collection is kept or changed as opposed to changing individual positions
				within the collections.
			</option>
			<option name="Iterations per feature">
				How often to sample the Shapley Value for each single feature.
				This directly affects the runtime of the loop since for each row in the first input table <i>1 + 2 * number of features * number of iterations per feature</i> rows are created
				and have to be predicted.
				The higher this number, the better the approximation but the longer the runtime.
			</option>
			<option name="Use seed">
				Using a seed allows to reproduce the results of the loop. If this box is checked the seed displayed in the text box is used, otherwise a new seed is generated for each
				execution. The new button allows to generate a new random seed.
				Note that the row sampling process (i.e. how rows are selected from the sampling table) is based on the quasi-random Sobol sequence and therefore actually deterministic (will be the same even if no seed is used).
				The random seed only applies to the permutation of features in algorithm 1 from <i>Explaining prediction models and individual predictions with feature contributions</i>
				by Strumbelj and Kononenko.
			</option>
		</tab>
		<tab name="Advanced Options">
			<option name="Chunk size">
				Since every row in the input may result in a very large number of rows in the output of the loop start node, this option allows
				to specify how many <b>input</b> rows should be handled per loop iteration.
				The output of the loop start will have <i>chunk size * (1 + 2 * number of features * number of iterations per feature)</i> rows
				(unless the number of rows left to process is smaller than chunk size).
			</option>
		</tab>
	</fullDescription>
	
	<ports>
		<inPort index="0" name="Table containing the rows to explain">
			Table containing the rows to explain.
		</inPort>
		<inPort index="1" name="Sampling data">
			Table containing rows used to perturb rows in the first table.
		</inPort>
		<outPort index="0" name="Perturbed rows">Perturbed rows.</outPort>
	</ports>
</knimeNode>
